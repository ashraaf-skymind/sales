{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recent-gauge",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deep_learning_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ee826ed072f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdeep_learning_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deep_learning_module'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#import packages needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "from torchsummaryX import summary\n",
    "\n",
    "# To auto load the customise module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import deep_learning_module\n",
    "import data_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/superstore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = df.loc[df['Category'] == 'Technology']\n",
    "technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology['Order Date'].min(), technology['Order Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-infrastructure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['Row ID', 'Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Quantity', 'Discount', 'Profit']\n",
    "technology.drop(cols, axis=1, inplace=True)\n",
    "technology.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology['Order Date'] = pd.to_datetime(technology['Order Date'])\n",
    "technology.set_index('Order Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = technology.groupby('Order Date')['Sales'].sum().reset_index()\n",
    "technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = technology.set_index('Order Date')\n",
    "technology.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = technology['Sales'].resample('MS').mean()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot(figsize=(20,10))\n",
    "plt.xlabel(\"Order Date\")\n",
    "plt.ylabel(\"Technology Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.7\n",
    "num_epochs = 60\n",
    "window_size = 2\n",
    "batch_size = 2\n",
    "n_step = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = round(len(y)*split_ratio)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data by indexing \n",
    "train_data = y[:split_data]\n",
    "test_data = y[split_data:]\n",
    "\n",
    "train_time = y.index[:split_data]\n",
    "test_time = y.index[split_data:]\n",
    "print(\"train_data_shape\")\n",
    "print(train_data.shape)\n",
    "print(\"test_data_shape\")\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the standard scaler, Use to fit the train data and take the statistic of train data of train data to apply in test data.\n",
    "scaler = StandardScaler().fit(train_data.values.reshape(-1, 1))\n",
    "train_data_standard = scaler.transform(train_data.values.reshape(-1, 1))\n",
    "test_data_standard = scaler.transform(test_data.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_data_standard shape : {train_data_standard.shape}\")\n",
    "print(f\"test_data_standard shape : {test_data_standard.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "trainX ,trainY =  data_module.univariate_multi_step(train_data_standard,window_size,n_step)\n",
    "testX , testY = data_module.univariate_multi_step(test_data_standard,window_size,n_step)\n",
    "### END SOLUTION\n",
    "print(f\"trainX shape:{trainX.shape} trainY shape:{trainY.shape}\\n\")\n",
    "print(f\"testX shape:{testX.shape} testY shape:{testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_assign(trainingX,testingX,trainingY,testingY):\n",
    "    \"\"\" \n",
    "    Use to assign  the key to create the train_data_dict and test_data_dict\n",
    "    \n",
    "    Arguments:\n",
    "    trainingX -- feature for traning data \n",
    "    testingX -- feature for testing data\n",
    "    trainingY -- label for traning data\n",
    "    testingY -- label for testing data\n",
    "    \n",
    "    Returns: \n",
    "    train_data_dict -- dictionary of trainingX and trainingY\n",
    "    test_data_dict -- dictionary of testingX and testingY\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # Create a dictionary that can store the train set feature and label\n",
    "    train_data_dict = {\"train_data_x_feature\" : trainingX, \"train_data_y_label\" : trainingY}\n",
    "    \n",
    "    # Create a dictionary that can store the test set feature and label\n",
    "    test_data_dict  = {\"test_data_x_feature\" : testingX , \"test_data_y_label\" : testingY }\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    return train_data_dict , test_data_dict\n",
    "\n",
    "train_data_dictionary , test_data_dictionary = key_assign(trainingX = trainX,\n",
    "                                 testingX = testX,\n",
    "                                 trainingY = trainY,\n",
    "                                 testingY = testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(train_data_dict, test_data_dict):\n",
    "    \"\"\" \n",
    "    Transform the NumPy data to torch tensor\n",
    "    \n",
    "    Arguments:\n",
    "    train_data_dict -- train data dictionary \n",
    "    test_data_dict -- test data dictionary\n",
    "    \n",
    "    Returns: \n",
    "    train_data_dict -- train data dictionary \n",
    "    test_data_dict -- test data dictionary\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    for train_datapoint in train_data_dict:\n",
    "        train_data_dict[train_datapoint] =  torch.from_numpy(train_data_dict[train_datapoint]).type(torch.Tensor)\n",
    "        \n",
    "    for test_datapoint in test_data_dict:\n",
    "        test_data_dict[test_datapoint] = torch.from_numpy(test_data_dict[test_datapoint]).type(torch.Tensor)\n",
    "        \n",
    "    ### END SOLUTION\n",
    "\n",
    "    return train_data_dict,test_data_dict\n",
    "\n",
    "train_data_dictionary,test_data_dictionary = transform(train_data_dictionary,test_data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-flesh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sanity_check(data_1,data_2):\n",
    "    \"\"\" \n",
    "    Print the shape of data_1 and data_2\n",
    "    \n",
    "    Arguments:\n",
    "    data_1 -- (dict) type of data\n",
    "    data_2 -- (dict) type of data \n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    for key_1 in data_1:\n",
    "        print(key_1 +\" shape : \" + str(data_1[key_1].shape))\n",
    "    for key_2 in data_2:\n",
    "        print(key_2 +\" shape : \" + str(data_2[key_2].shape))\n",
    "        \n",
    "    ### END SOLUTION\n",
    "# Sanity check\n",
    "sanity_check(train_data_dictionary,test_data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iterator\n",
    "def iterator(train_data_dict,test_data_dict,batch_size):\n",
    "    \"\"\" \n",
    "    Create iterator for train data and test data \n",
    "    \n",
    "    Arguments:\n",
    "    train_data_dict -- train data dictionary \n",
    "    test_data_dict -- test data dictionary\n",
    "    \n",
    "    Returns: \n",
    "    train_iter -- train data iterator \n",
    "    test_iter -- test data iterator \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    train_dataset = TensorDataset(train_data_dict[\"train_data_x_feature\" ],\n",
    "                                  train_data_dict[\"train_data_y_label\"])\n",
    "    train_iter = DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    test_dataset = TensorDataset(test_data_dict[\"test_data_x_feature\"],\n",
    "                                 test_data_dict[\"test_data_y_label\"])\n",
    "    test_iter = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return train_iter , test_iter\n",
    "\n",
    "train_iter , test_iter = iterator(train_data_dictionary,test_data_dictionary,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#Arguments for LSTM model\n",
    "hidden_dim = 1\n",
    "n_feature = 1 \n",
    "n_step = 2\n",
    "\n",
    "#1 for vanila LSTM , >1 is mean stacked LSTM\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "#Vanila , Stacked LSTM\n",
    "model = deep_learning_module.LSTM(n_feature = n_feature ,\n",
    "                         hidden_dim = hidden_dim ,\n",
    "                         num_layers = num_layers,\n",
    "                         n_step = n_step)\n",
    "#Bidirectional LSTM\n",
    "# model = deep_learning_module.BidirectionalLSTM(n_feature = n_feature ,\n",
    "#                          hidden_dim = hidden_dim ,\n",
    "#                          num_layers = num_layers,\n",
    "#                          n_step = n_step)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#optimiser\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#dropout\n",
    "# dropout = nn.Dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-automation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = torch.zeros((batch_size,window_size,1),dtype=torch.float) # batch size , seq_length , input_dim\n",
    "print(summary(model,inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training \n",
    "torch.manual_seed(123)\n",
    "train_loss,val_loss = deep_learning_module.training(num_epochs,train_iter,test_iter,optimizer,loss_fn,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-cause",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_module.learning_curve(num_epochs,train_loss,val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.zoom_learning_curve(start_epoch = 50,\n",
    "                                end_epoch =60,\n",
    "                                training_loss = train_loss,\n",
    "                                validation_loss = val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 : Feed in the train and test data to the model\n",
    "with torch.no_grad():\n",
    "    y_train_prediction = model(train_data_dictionary[\"train_data_x_feature\"])\n",
    "    y_test_prediction = model(test_data_dictionary[\"test_data_x_feature\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_assign_evaluation(y_train_prediction,\n",
    "                          y_test_prediction,\n",
    "                          train_data_dictionary,\n",
    "                          test_data_dictionary):\n",
    "    \"\"\" \n",
    "    Assign key for prediction and output data dictionary \n",
    "    \n",
    "    Arguments:\n",
    "    y_train_prediction -- (tensor) prediction for training data\n",
    "    y_test_prediction -- (tensor) prediction for test data\n",
    "    train_data_dictionary -- (dict) train data dictionary\n",
    "    test_data_dictionary -- (dict) test data dictionary\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "    prediction -- (dict) dictionary that consists of prediction from train data and test data\n",
    "    output_data -- (dict) dictionary that consists of output(label) from train data and test data\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    prediction ={\"train_data_prediction\" : y_train_prediction,\n",
    "            \"test_data_prediction\" :y_test_prediction }\n",
    "    output_data ={\"train_data_output\" : train_data_dictionary[\"train_data_y_label\"] ,\n",
    "               \"test_data_output\" : test_data_dictionary[\"test_data_y_label\"]}\n",
    "    ### END SOLUTION\n",
    "    return prediction , output_data\n",
    "\n",
    "prediction , output_data = key_assign_evaluation(y_train_prediction,y_test_prediction,\n",
    "                                                 train_data_dictionary,\n",
    "                                                 test_data_dictionary)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction and output shape\n",
    "sanity_check(data_1 = prediction,data_2 = output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 : Reshape both to the original data dimension\n",
    "def squeeze_dimension(output):\n",
    "    \"\"\" \n",
    "    Squeeze the dimension of output data\n",
    "    \n",
    "    Arguments:\n",
    "    output -- (dict) output_data\n",
    "    \n",
    "    Returns: \n",
    "    output_data -- (dict) output_data\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    for key in output:\n",
    "        output[key] = torch.squeeze(output[key],2)\n",
    "    ### END SOLUTION\n",
    "    return output\n",
    "\n",
    "output_data = squeeze_dimension(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output shape\n",
    "sanity_check(data_1 = output_data,data_2 = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 : Invert the scaling back to orignal data value\n",
    "def inverse_scaler(scaled_data,scaler):\n",
    "    \"\"\" \n",
    "    Inverse the scaled data\n",
    "    \n",
    "    Arguments:\n",
    "    scaled_data -- (dict) data that being scaled \n",
    "    scaler -- scaler \n",
    "    \n",
    "    Returns: \n",
    "    scaled_data -- (dict) data after inverse scale\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    for item in scaled_data:\n",
    "        scaled_data[item] =  scaler.inverse_transform(scaled_data[item].detach().numpy())\n",
    "    ### END SOLUTION\n",
    "    return scaled_data\n",
    "    \n",
    "prediction = inverse_scaler(prediction,scaler)\n",
    "output_data  = inverse_scaler(output_data ,scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(data_1 = prediction,data_2 = output_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_forecast_value(output_data,prediction):\n",
    "    \"\"\" \n",
    "    To list the test output and prediction output side by side\n",
    "    \n",
    "    Arguments:\n",
    "    output_data --  (dict) output data dictionary\n",
    "    prediction -- (dict) prediction output dictionary\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    print(\"Test Data\\t\\t\\tForecast\")\n",
    "    for test, forecast in zip(output_data[\"test_data_output\"],prediction[\"test_data_prediction\"]):   \n",
    "        print(f\"{test}\\t\\t{forecast}\")\n",
    "    ### END SOLUTION\n",
    "        \n",
    "list_forecast_value(output_data,prediction)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 : Calculate the RMSE of train and test data\n",
    "def rmse(prediction,output_data):\n",
    "    \"\"\" \n",
    "    Calculate RMSE between output data and prediction data \n",
    "    \n",
    "    Arguments:\n",
    "    prediction -- (dict) prediction output dictionary\n",
    "    output_data --  (dict) output data dictionary\n",
    "    \n",
    "    Returns:\n",
    "    trainScore - RMSE of train dataset\n",
    "    testScore - RMSE of test dataset\n",
    "    \"\"\"\n",
    "    trainScore = math.sqrt(mean_squared_error(prediction[\"train_data_prediction\"], output_data[\"train_data_output\"]))\n",
    "    testScore = math.sqrt(mean_squared_error(prediction[\"test_data_prediction\"], output_data[\"test_data_output\"]))\n",
    "    return trainScore,testScore\n",
    "\n",
    "trainScore,testScore = rmse(prediction,output_data)\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_details ={\"x-axis\" : \"Date\",\n",
    "          \"y-axis\" : \"Values\",\n",
    "          \"title\"  : \"Technology Sales\"\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecast plot for multi-step\n",
    "def multi_step_plot(original_test_data,\n",
    "                    after_sequence_test_data ,\n",
    "                    forecast_data,test_time,window_size,\n",
    "                    n_step ,\n",
    "                    details = {},\n",
    "                    original_plot = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    Plot the result of the multi-step forecast \n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    original_test_data -- test data before sequence\n",
    "    \n",
    "    after_sequence_test_data -- (dict) output data dictionary\n",
    "    \n",
    "    forecast_data -- (dict) prediction data dictionary\n",
    "    \n",
    "    test_time --  time index for test data before sliding window (data sequence)\n",
    "    \n",
    "    window_size -- window size for the data sequence\n",
    "    \n",
    "    n_step -- the number of future step , 1 -> single >1 -> multi-step\n",
    "    \n",
    "    details -- (dict) details for plot such as \"x-axis\" ,\"y-axis\", \"title\"\n",
    "    \n",
    "    original_plot -- (boolean) True ->observe how sliding window (data sequence) take place in the test data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    after_sequence_test_data = after_sequence_test_data['test_data_output'] \n",
    "    forecast_data = forecast_data[\"test_data_prediction\"]\n",
    "    \n",
    "    # Plot Setting\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.xticks(rotation=45)    \n",
    "    \n",
    "    # Store test and forecast data into DataFrame type \n",
    "    column_names = [\"timestep_\" + str(i) for i in range(after_sequence_test_data.shape[1])]\n",
    "    y_test_dataframe = pd.DataFrame(after_sequence_test_data,columns = column_names)\n",
    "    y_test_pred_dataframe =pd.DataFrame(forecast_data,columns = column_names)\n",
    "    \n",
    "    # Create time index for data after sequence\n",
    "    time_index_after_sequence = test_time[window_size:]\n",
    "    \n",
    "    # Test Data plot before sliding window(data sequencing)\n",
    "    if original_plot:\n",
    "        plt.plot(test_time,original_test_data,marker='x',color=\"blue\")\n",
    "\n",
    "    # For loop to plot the data step by step base on time index    \n",
    "    start_idx = 0 \n",
    "    for row in range(len(y_test_dataframe)):\n",
    "        \n",
    "        # Iterate the time index after sequence\n",
    "        time_index = time_index_after_sequence[start_idx:start_idx+n_step]\n",
    "        \n",
    "        # Plot the test data\n",
    "        plt.plot(time_index,y_test_dataframe.iloc[row],color=\"green\",marker='o')\n",
    "        \n",
    "        # Plot the forecast data\n",
    "        plt.plot(time_index,y_test_pred_dataframe.iloc[row],color=\"red\",marker='o')\n",
    "        \n",
    "        # Pointer for time_index_after_sequence\n",
    "        start_idx += 1\n",
    "        \n",
    "    # Customize the legend\n",
    "    custom_lines = [Line2D([0], [0], color=\"green\", lw=4),\n",
    "                Line2D([0], [0], color=\"red\", lw=4),\n",
    "                Line2D([0], [0], color=\"blue\", lw=4)]\n",
    "    plt.legend(custom_lines, ['Test Data After Sequencing', 'Forecast Data', 'Test Data Before Sequencing'])\n",
    "    \n",
    "    # Extra details - Optional function\n",
    "    if details != {}:\n",
    "        plt.xlabel(details[\"x-axis\"])\n",
    "        plt.ylabel(details[\"y-axis\"])\n",
    "        plt.title(details[\"title\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the multi_step_plot function\n",
    "multi_step_plot(original_test_data = test_data,\n",
    "                after_sequence_test_data = output_data ,\n",
    "                forecast_data = prediction,\n",
    "                test_time = test_time,\n",
    "                window_size = window_size ,\n",
    "                n_step = n_step,\n",
    "                details = plot_details,\n",
    "                original_plot = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
